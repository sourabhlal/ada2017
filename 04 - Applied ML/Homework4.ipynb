{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?\n",
    "#### Answer : \n",
    "\n",
    "We start by loading the data and viewing the first few observations. The data consists of 614 observations, for which we have 11 variables.\n",
    "We proceed to dividing the data into two dataframes :\n",
    "- **data_treat** : containing the 185 individuals who followed the training.\n",
    "- **data_control** : containing the 429 individuals who didn't follow the training.\n",
    "\n",
    "We note that *training* and *treatment* are used indifferently to describe the job training program (\"National Supported Work Demonstration\") that we are investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  \n",
       "0   9930.0460  \n",
       "1   3595.8940  \n",
       "2  24909.4500  \n",
       "3   7506.1460  \n",
       "4    289.7899  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "data = pd.read_csv(\"lalonde.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 11)\n",
      "(429, 11)\n"
     ]
    }
   ],
   "source": [
    "#Split data into two dataframes (treatment and control)\n",
    "data_treat = data[data.treat==1]\n",
    "data_control = data[data.treat==0]\n",
    "\n",
    "\n",
    "print(data_treat.shape)\n",
    "print(data_control.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A fast and easy way to check whether the treatment had any effect on the real earnings of those who followed it, is to compute the distribution of the outcome variable (re78) and compare it between both groups.\n",
    "Boxplots are used here to view these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('cubehelix',2)\n",
    "\n",
    "#figure\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.boxplot([data_control.re78,data_treat.re78],0,'rD')\n",
    "\n",
    "#labels\n",
    "plt.ylabel('Real earnings')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real earnings distributions for the year 1978')\n",
    "plt.xticks([1,2],['control','treatment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the distributions of the two groups is not as striking that one would expect. We observe that the medians are quite similar and both distrubutions have a longer tail towards the higher earnings. This is most likely due to the fact that some people have real earnings equal to zero. We also observe that the treatment group have a few more outiers than the control group.\n",
    "\n",
    "To confirm the results of this figure, we create a table containing the numerical values of these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Descriptive statistics \n",
    "descrip = pd.DataFrame()\n",
    "descrip['control'] = data_control['re78'].describe()\n",
    "descrip['treatment'] = data_treat['re78'].describe()\n",
    "np.round(descrip, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, we see that both the median and the mean are actually higher in the control group. Although the treatment group has some big outliers (as observed on the boxplot), the control group has a larger 75% quantile which means there are more people in this group having larger salaries.\n",
    "We also notice that the treatment group has real earnings of value zero, which means that some of the trained people ended up unemployed anyway.\n",
    "In the light of this superficial analysis, one might conclude that the training didn't work, or even decreased people's chances at a better salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A propensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores.\n",
    "\n",
    "#### Answer :\n",
    "\n",
    "We will now perform a deeper analysis of the problem. Indeed, since this data is collected from an observational study, the assignment process into treatment or control groups might not have been completely random. In order to check this, we will study the distributions of the rest of the variables and try to detect any differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#figure\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "#boxplot for real ernings 1974\n",
    "plt.subplot(221)\n",
    "plt.boxplot([data_control.re74,data_treat.re74],0, 'rD')\n",
    "plt.ylabel('Real Earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real Earning distributions for the year 1974',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for real earnings 1975\n",
    "plt.subplot(222)\n",
    "plt.boxplot([data_control.re75,data_treat.re75],0, 'rD')\n",
    "plt.ylabel('Real Earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real Earning distributions for the year 1975',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for age\n",
    "plt.subplot(223)\n",
    "plt.boxplot([data_control.age,data_treat.age], 0, 'bD')    \n",
    "plt.title('Age distribution',size=20)\n",
    "plt.ylabel('age')\n",
    "plt.xlabel('Group')    \n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for education\n",
    "plt.subplot(224)\n",
    "plt.boxplot([data_control.educ,data_treat.educ], 0, 'gD')    \n",
    "plt.title('Education distribution',size=20)\n",
    "plt.ylabel('years of education')\n",
    "plt.xlabel('Group')    \n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with boxplots of the real earnings for the years preceeding the treatment.\n",
    "From the boxplots above, we can clearly see that the medians in the treatment groups are equal to zero, which means that at least half of the group was unemployed before the treatment. The control group however shows a positive median, as well as  larger 75% quantiles. Although the treatment group shows some extreme outliers, it seems that overall, the people who are involved tend to have lower initial salaries.\n",
    "This might be interpreted as a discriminating factor between the groups: since they didn't start at the same level of income, they cannot be fairly compared after the treatment. Thus we conclude that the naive analysis we did in the previous question is in fact invalid.\n",
    "\n",
    "The distributions for age and eduction are not too different (i.e we cannot say that one group is younger than the other or more educated), but the fact that the control group is larger (429) makes it cover a broader range of age and education values. \n",
    "\n",
    "Finally, we compute the proportion of each categorical variable in the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the number of black/hispanic/married/nodegree person in each group\n",
    "control_ratio = data_control.iloc[:,4:8].sum()\n",
    "treat_ratio = data_treat.iloc[:,4:8].sum()\n",
    "\n",
    "#divide it by the total size of the group \n",
    "ratio = pd.DataFrame([100*control_ratio/len(data_control),100*treat_ratio/len(data_treat)])\n",
    "ratio.index = ['control','treatment']\n",
    "np.round(ratio,decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratios are not equal for both groups, the most striking difference can be seen in the proportion of black people (20% of the control group and more than 80% of the treatment group ).\n",
    "This confirms that the control and treatment groups do not represent the same population and thus the effect of the treatment cannot be judged accurately when compared to this control group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer : \n",
    "In this section, we will compute the propensity scores via logistic regression in order to estimate the probability for each person of receiving the treatment given their known characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitting logistic model to relevant parameters\n",
    "covariates =data[['age', 'educ', 'black', 'hispan', 'married', 'nodegree','re74','re75']]\n",
    "X = covariates\n",
    "Z = data.treat\n",
    "logistic.fit(X,Z)\n",
    "logistic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the coefficients of the logistic model\n",
    "\n",
    "Now that we have trained the model on the dataset, we will use it to compute the propensity scores for each observation. Then we add the values to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "propensity = logistic.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding the probabilities for each individual to the dataframe\n",
    "prop = pd.DataFrame(propensity)\n",
    "prop.columns = ['prob_control' ,'prob_treat']\n",
    "data = pd.concat([data,prop['prob_treat']],axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "In order to perfom the matching, we must first translate our data into a bipartite graph C $\\cup$ T, where each node is an observation from the data, C is the subset of control observations and T is the subset of treatment observations.\n",
    "We assign to each node a list of attributes that will help construct the edges. For now these attributes consists of the propensity score and an indicator of wether the person was treated or not.\n",
    "\n",
    "Our matching aims at linking together pairs of control and treatment observations which have close propensity scores.\n",
    "\n",
    "For this purpose we create edges linking every treatment node to all control nodes and assign each edge a weight defined as the absolute difference between the propensities of the nodes it connects.\n",
    "As we want to minimize these differences we simply add a minus to the absolute difference and then apply a maximum weighted matching to our graph.\n",
    "\n",
    "Note that since edges only exist between nodes of different subsets the graph is bipartite as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "#for each node we add relevant attributes \n",
    "for k in range(len(data)):\n",
    "        G.add_node(k, idf=data.id[k], prop=data.prob_treat[k], treat=data.treat[k])\n",
    "\n",
    "#print an example         \n",
    "print(G.nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for every treatment node we find all control nodes and add an edge with the desired weight \n",
    "for i in G.nodes():\n",
    "    if G.nodes[i]['treat'] == 1:\n",
    "        for j in G.nodes():\n",
    "            if G.nodes[j]['treat'] == 0:\n",
    "                W = -abs(G.nodes[i]['prop'] - G.nodes[j]['prop'])\n",
    "                G.add_edge(i, j, weight=W )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the graph we just created, we compute a few important numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print key numbers\n",
    "print ('Number of nodes in the graph: {}'.format(G.number_of_nodes()))\n",
    "print('Number of edges in the graph: {}'.format(G.number_of_edges()))\n",
    "\n",
    "#We check if the graph is bipartite: \n",
    "if (nx.is_bipartite(G)):\n",
    "    print( 'The graph is bipartite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the graph is bipartite and contains the same amount of nodes as datapoints in our dataset. The amount of edges is supposed to be:\n",
    "\n",
    "$N_{treat} \\cdot N_{control} = 185 \\cdot 429 = 79.365$ \n",
    "\n",
    "Which is (luckily) the amount of edges the we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to do the actual matching. This will correspond to keeping the edges that yeilds the highest sum of weights (recall the weight is minus the absolute difference of propensity scores) and at the same time only keeping one edge between any nodes.\n",
    "\n",
    "In practice this is done using the networkX function shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matching=nx.max_weight_matching(G,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#subset of treated observations\n",
    "T = data[data.treat==1]\n",
    "\n",
    "#subset of control observations\n",
    "#initializing dataframe\n",
    "C = pd.DataFrame([])\n",
    "\n",
    "#adding rows from data where index is equal to the node matched to each treatment node\n",
    "for k in range(0,185):\n",
    "    C = pd.concat([C,data[data.index == matching[k]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine whether the resulting groups are now more balanced than the initial ones, we take a look at the descrptive statistics and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(T.describe(), decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(C.describe(), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for most variables the means and the medians are quite similar. We note that the average propensity score is different in the treatment and control group. To examine the varables further, we look at the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "#boxplot for real earning 1974\n",
    "plt.subplot(221)\n",
    "plt.boxplot([C.re74,T.re74],0, 'rD')\n",
    "plt.ylabel('Real Earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real Earning distributions for the year 1974',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for real earnings 1975\n",
    "plt.subplot(222)\n",
    "plt.boxplot([C.re75,T.re75],0, 'rD')\n",
    "plt.ylabel('Real Earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real Earning distributions for the year 1975',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for age\n",
    "plt.subplot(223)\n",
    "plt.boxplot([C.age,T.age], 0, 'bD')    \n",
    "plt.ylabel('age')\n",
    "plt.xlabel('Group')\n",
    "plt.title('Age distribution',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for education\n",
    "plt.subplot(224)\n",
    "plt.boxplot([C.educ,T.educ], 0, 'gD')    \n",
    "plt.ylabel('years of education')\n",
    "plt.xlabel('Group')\n",
    "plt.title('Education distribution',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the real earnings of 1974 and 1975 are now much more similar for the two groups. However for both education and age the differences seems to be larger now.\n",
    "\n",
    "We also want to see if the categorical values are similar enough after the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the number of black /hispanic/married/nodegree person in each group\n",
    "C_ratio=C.iloc[:,4:8].sum()\n",
    "T_ratio=T.iloc[:,4:8].sum()\n",
    "\n",
    "# divide it by the total size of the group \n",
    "ratio=pd.DataFrame([100*C_ratio/len(C),100*T_ratio/len(T)])\n",
    "ratio.index=['control','treatment']\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in general they are now quite similar except for the ratio of black people, that is still way higher in the treatment group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the outcome (namely the real earnings in 1978) looks different after the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a figure\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.boxplot([C.re78,T.re78],0, 'rD')\n",
    "plt.ylabel('Real earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real earnings distributions for the year 1978')\n",
    "plt.xticks([1,2],['control','treatment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the real earnings for the treatment group is now better than for the control group. The median seems to be a bit higher and there are also more very high values. Since we noticed that the goups are still not completely balanced, we need to investigate further before concluding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "We will now try to improve the matching further based on the implications that were observed in the previous part. The two problematic features found in the previous part are the age and the ratio of black people. We will now try to fix these issue. We do that by creating a graph again with all datapoints as nodes and only adding edges between treated and control nodes that:\n",
    " - share the same value for the attribute 'black'\n",
    " - have an age difference smaller than two years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we create a new graph, this time also adding black and age as an attribute to the nodes\n",
    "G2 = nx.Graph()\n",
    "for k in range(len(data)):\n",
    "        G2.add_node(k, idf=data.id[k],prop= data.prob_treat[k],treat=data.treat[k],\n",
    "                    black=data.black[k],age=data.age[k])\n",
    "\n",
    "#print an example\n",
    "G2.nodes[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we add edges between the nodes\n",
    "for i in G2.nodes():\n",
    "    if G2.nodes[i]['treat']==1:\n",
    "        for j in G2.nodes():\n",
    "            if ((G2.nodes[j]['treat']==0) & (G2.nodes[j]['black'] == G2.nodes[i]['black']) \n",
    "                & ((abs(G2.nodes[j]['age']-G2.nodes[i]['age']))<2)):\n",
    "                    W = -abs(G2.nodes[i]['prop'] - G2.nodes[j]['prop'])\n",
    "                    G2.add_edge(i, j, weight=W )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print key numbers\n",
    "print ('number of nodes in the graph: '+str(G2.number_of_nodes()))\n",
    "print('number of edges in the graph: '+str(G2.number_of_edges()))\n",
    "#We check if the graph is bipartite: \n",
    "if (nx.is_bipartite(G2)):\n",
    "    print( 'the graph is bipartite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note that the amount of edges has decreased quite a lot. This is due to the fact that every treatment node is no longer matched to all control nodes, but only to the control nodes with which it shares the same value for 'black' (wether the person is black or not) and an age difference smaller than two.\n",
    "\n",
    "We do a new round of matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we do the max. weight matching \n",
    "matching=nx.max_weight_matching(G2,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize dataframe for control and treatment observations\n",
    "C2 = pd.DataFrame([])\n",
    "T2 = pd.DataFrame([])\n",
    "\n",
    "#adding rows from data\n",
    "for k in range(0,185):\n",
    "    if k in matching:\n",
    "        C2 = pd.concat([C2,data[data.index == matching[k]]])\n",
    "        T2 = pd.concat([T2,data[data.index == k]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the descriptive statistcs for the two groups again to see if they are more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(T2.describe(), decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(C2.describe(), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular we notice that the average propensity score is now close to 0.5 for both groups. This means we have reached a matching that reasembles a random group assignment. The distribution of the other features are further examined through boxplots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.boxplot([C2.re74,T2.re74],0, 'rD')\n",
    "plt.ylabel('Real Earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real Earning distributions for the year 1974',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.boxplot([C2.re75,T2.re75],0, 'rD')\n",
    "plt.ylabel('Real Earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real Earning distributions for the year 1975',size=20)\n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "#boxplot for\n",
    "plt.subplot(223)\n",
    "plt.boxplot([C2.age,T2.age], 0, 'bD')    \n",
    "plt.title('Age distribution',size=20)\n",
    "plt.ylabel('age')\n",
    "plt.xlabel('Group')    \n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.boxplot([C2.educ,T2.educ], 0, 'gD')    \n",
    "plt.title('Education distribution',size=20)\n",
    "plt.ylabel('years of education')\n",
    "plt.xlabel('Group')    \n",
    "plt.xticks([1,2],['control','treatment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the distributions for age and education is now similar (considering a one year difference not very important). The real earnings distribution however are now not as similar as in the previous matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ratios \n",
    "#compute the number of black /hispanic/married/nodegree person in each group\n",
    "C2_ratio=C2.iloc[:,4:8].sum()\n",
    "T2_ratio=T2.iloc[:,4:8].sum()\n",
    "\n",
    "#divide it by the total size of the group \n",
    "ratio=pd.DataFrame([100*C2_ratio/len(C2),100*T2_ratio/len(T2)])\n",
    "ratio.index=['control','treatment']\n",
    "np.round(ratio, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our method succeeded in levelling the black ratios while keeping the rest of the ratios reasonably balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "As the propensity scores are much better and the distribution of the problematic features are improved we decide to keep this matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n",
    "\n",
    "#### Answer : \n",
    "We now ready to examine the distribution of the real earnings in 1978 for both groups created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a figure and 1x3 subplots \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.boxplot([C2.re78,T2.re78],0, 'rD')\n",
    "plt.ylabel('Real earning')\n",
    "plt.xlabel('Group')    \n",
    "plt.title('Real earnings distributions for the year 1978',size=15)\n",
    "plt.xticks([1,2],['control','treatment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The real earning in 1978 were {}$ higher for the treatment group on average'.format(round(T2.re78.median() - C2.re78.median())))\n",
    "print('The real ernings of the participants in the program improved by {}$ between 1975 and 1978 on average'.format(round(T2.re78.median() - T2.re75.median())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the naive analysis, the matched analysis allows us to conclude that the job trainning program actually had a positive effect on the participants real earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the articles from the sklearn dataset. We can see the 20 categories that we are going to cluster the articles into. We decided to get the data in a random order so that we do not have to randomize the order later when splitting the data into training, testing and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 categories are:\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#Fetch data. We download the data if it is not saved already.\n",
    "newsgroups20 = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, download_if_missing=True)\n",
    "\n",
    "#20 news categories\n",
    "categories = newsgroups20.target_names\n",
    "\n",
    "#printing it!\n",
    "print (\"The 20 categories are:\")\n",
    "pprint(list(newsgroups20.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 282387)\t0.0140258057605\n",
      "  (0, 365517)\t0.113522139385\n",
      "  (0, 237867)\t0.11127683307\n",
      "  (0, 450247)\t0.110280777821\n",
      "  (0, 123734)\t0.117802347049\n",
      "  (0, 423435)\t0.430992321225\n",
      "  (0, 268816)\t0.147765943013\n",
      "  (0, 451143)\t0.121668965308\n",
      "  (0, 434140)\t0.0520731444266\n",
      "  (0, 405733)\t0.0896450297201\n",
      "  (0, 197952)\t0.0714324612804\n",
      "  (0, 373078)\t0.0757326847264\n",
      "  (0, 428627)\t0.0729556662549\n",
      "  (0, 417615)\t0.0744050019808\n",
      "  (0, 77954)\t0.083588891865\n",
      "  (0, 431478)\t0.136434968273\n",
      "  (0, 150033)\t0.108705621629\n",
      "  (0, 506414)\t0.0461762191086\n",
      "  (0, 490927)\t0.0306860931226\n",
      "  (0, 173795)\t0.13098527592\n",
      "  (0, 402464)\t0.0926395655756\n",
      "  (0, 160883)\t0.0460336953254\n",
      "  (0, 437567)\t0.0542189115102\n",
      "  (0, 216675)\t0.0854213348028\n",
      "  (0, 137875)\t0.0557677685832\n",
      "  :\t:\n",
      "  (18845, 273981)\t0.102179157679\n",
      "  (18845, 104201)\t0.102179157679\n",
      "  (18845, 94404)\t0.102179157679\n",
      "  (18845, 225732)\t0.102179157679\n",
      "  (18845, 48685)\t0.102179157679\n",
      "  (18845, 31750)\t0.102179157679\n",
      "  (18845, 31749)\t0.102179157679\n",
      "  (18845, 153856)\t0.102179157679\n",
      "  (18845, 154697)\t0.102179157679\n",
      "  (18845, 370920)\t0.102179157679\n",
      "  (18845, 28465)\t0.102179157679\n",
      "  (18845, 494076)\t0.102179157679\n",
      "  (18845, 198667)\t0.102179157679\n",
      "  (18845, 97563)\t0.102179157679\n",
      "  (18845, 450179)\t0.102179157679\n",
      "  (18845, 491272)\t0.102179157679\n",
      "  (18845, 530185)\t0.102179157679\n",
      "  (18845, 490884)\t0.102179157679\n",
      "  (18845, 486379)\t0.102179157679\n",
      "  (18845, 527127)\t0.102179157679\n",
      "  (18845, 420469)\t0.102179157679\n",
      "  (18845, 433719)\t0.102179157679\n",
      "  (18845, 576275)\t0.102179157679\n",
      "  (18845, 530624)\t0.102179157679\n",
      "  (18845, 467806)\t0.102179157679\n"
     ]
    }
   ],
   "source": [
    "#Function to separate the documents into tokens by splitting on spaces and changing all characters to lowercase.\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "# We create the sklearn TFIDF vectorizer, and compute the tfidf features of the data.\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "tfidf_results = tfidf.fit_transform(newsgroups20.data)\n",
    "\n",
    "print(tfidf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we separate the data into training, testing and validation data sets. We don't randomize it since we loaded the data in random order earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15072, 591946) (1884, 591946) (1890, 591946) (15072,) (1884,) (1890,)\n"
     ]
    }
   ],
   "source": [
    "data_len = len(newsgroups20.data)\n",
    "\n",
    "data_80 = 8 * int(data_len / 10)\n",
    "data_90 = 9 * int(data_len / 10)\n",
    "\n",
    "#80% training data\n",
    "train_tfidf = tfidf_results[:data_80]\n",
    "train_data = newsgroups20['target'][:data_80]\n",
    "\n",
    "#10% testing data\n",
    "test_tfidf = tfidf_results[data_80:data_90]\n",
    "test_data = newsgroups20['target'][data_80:data_90]\n",
    "\n",
    "#10% validation data\n",
    "val_tfidf = tfidf_results[data_90:]\n",
    "val_data = newsgroups20['target'][data_90:]\n",
    "\n",
    "print(train_tfidf.shape, test_tfidf.shape, val_tfidf.shape, train_data.shape, test_data.shape, val_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV ,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56104033970276013"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(train_tfidf,train_data)\n",
    "init_training_prediction = model.predict(test_tfidf)\n",
    "init_training_score = accuracy_score(init_training_prediction,test_data)\n",
    "init_training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testRandomForest(depth, n_estimator):\n",
    "        random_forest = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator, random_state=0)\n",
    "        random_forest.fit(train_tfidf,train_data)\n",
    "        prediction = random_forest.predict(val_tfidf)\n",
    "        print(\"depth: \"+str(depth)+\"\\t n_estimators: \"+str(n_estimator)+\"\\t accuracy: \"+str(accuracy_score(val_data, prediction)))\n",
    "        return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 5\t n_estimators: 800\t accuracy: 0.646560846561\n",
      "depth: 5\t n_estimators: 900\t accuracy: 0.654497354497\n",
      "depth: 5\t n_estimators: 1000\t accuracy: 0.657671957672\n",
      "depth: 10\t n_estimators: 800\t accuracy: 0.703703703704\n",
      "depth: 10\t n_estimators: 900\t accuracy: 0.706878306878\n",
      "depth: 10\t n_estimators: 1000\t accuracy: 0.706349206349\n"
     ]
    }
   ],
   "source": [
    "depth_init = 5\n",
    "depth_max = 10\n",
    "estim_init = 800\n",
    "estim_max = 1000\n",
    "        \n",
    "for depth in range(depth_init, depth_max+1,5):\n",
    "    for n_estimator in range(estim_init, estim_max+1,100):\n",
    "        testRandomForest(depth, n_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 50\t n_estimators: 1750\t accuracy: 0.798412698413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trained_classifier.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_classifier = testRandomForest(50, 1750)\n",
    "\n",
    "#dump to pickle\n",
    "joblib.dump(trained_classifier, 'trained_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load from pickle\n",
    "trained_classifier = joblib.load('trained_classifier.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = trained_classifier.predict(test_tfidf)\n",
    "test_score = accuracy_score(test_prediction,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77547770700636942"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46  0  0  0  0  0  5  0  0  0  1  0  1  1  2 15  1  2  0  5]\n",
      " [ 0 54  9  3  0 11 16  0  0  0  0  0  1  0  0  0  1  0  1  0]\n",
      " [ 0  1 80  7  2  6  9  0  0  0  0  1  3  0  0  0  0  0  0  0]\n",
      " [ 1  2  6 58  3  4 19  0  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  1  2  5 70  2 13  1  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  2  4  0  0 88  2  0  0  0  1  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  1  1 93  2  0  0  2  1  2  0  0  1  0  0  0  0]\n",
      " [ 0  2  1  1  0  0 12 95  4  2  0  0  2  1  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  8  2 81  0  0  1  0  0  1  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  1  6  0  0 89  8  0  0  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0  1 99  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  2  0  0  1  0 99  4  0  0  0  0  0  0  0]\n",
      " [ 0  3  2  2  1  1 10  1  0  1  1  1 53  0  3  1  1  0  0  0]\n",
      " [ 0  2  0  1  1  1 11  0  1  1  0  0  6 66  1  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  1  9  0  0  0  0  0  1  1 72  0  2  0  0  0]\n",
      " [ 1  1  0  0  0  0  4  0  0  0  0  0  1  0  1 86  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  0  0  0  0  4  2  0  0  3 73  0  0  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  0  0  0  1  0  0  1  0 83  1  0]\n",
      " [ 0  0  0  2  0  0  1  0  0  2  2  2  1  4  1  7  6  3 51  0]\n",
      " [ 1  0  0  0  1  0  2  1  0  1  0  0  2  3  0 16  8  1  0 25]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_data , test_prediction)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.58      0.72        79\n",
      "          1       0.72      0.56      0.63        96\n",
      "          2       0.77      0.73      0.75       109\n",
      "          3       0.72      0.61      0.66        95\n",
      "          4       0.89      0.74      0.80        95\n",
      "          5       0.75      0.90      0.82        98\n",
      "          6       0.40      0.89      0.55       105\n",
      "          7       0.93      0.79      0.85       121\n",
      "          8       0.94      0.87      0.91        93\n",
      "          9       0.91      0.82      0.86       108\n",
      "         10       0.87      0.94      0.90       105\n",
      "         11       0.91      0.93      0.92       107\n",
      "         12       0.64      0.65      0.65        81\n",
      "         13       0.85      0.73      0.78        91\n",
      "         14       0.88      0.80      0.84        90\n",
      "         15       0.66      0.91      0.76        94\n",
      "         16       0.79      0.86      0.82        85\n",
      "         17       0.93      0.93      0.93        89\n",
      "         18       0.96      0.62      0.76        82\n",
      "         19       0.83      0.41      0.55        61\n",
      "\n",
      "avg / total       0.81      0.78      0.78      1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data , test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortedFeatures = np.argsort(trained_classifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"sortedFeatures.pickle\",\"wb\")\n",
    "pickle.dump(sortedFeatures, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_in = open(\"sortedFeatures.pickle\",\"rb\")\n",
    "sortedFeatures = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([571038, 208577, 181350, ..., 323765, 323768, 295972])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedFeatures[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f3a855373449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = np.array(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
