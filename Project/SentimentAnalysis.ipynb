{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Subcategory Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date, time\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import IBM Watson Libraries, and add API credentials. Note: Real credentials not included here as this is a public repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, KeywordsOptions, EntitiesOptions, EmotionOptions, SentimentOptions\n",
    "\n",
    "ibm_pass = \"pwQydFvZpz5S\"\n",
    "ibm_user = \"0b421165-dcf4-4050-8a15-30a87e0bd498\"\n",
    "ibm = {'u':ibm_user, 'p':ibm_pass}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`getSentiment` is a function to make API requests and parse the responses.\n",
    "\n",
    "`getNLU` is a function to compute the number of credits being used for a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(text):\n",
    "    if len(text) > 20:\n",
    "        natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "          username=ibm['u'],\n",
    "          password=ibm['p'],\n",
    "          version=\"2017-02-27\")\n",
    "        try:\n",
    "            response = natural_language_understanding.analyze(text=text,features=Features(sentiment=SentimentOptions(document=True)))\n",
    "            report = response[\"sentiment\"][\"document\"][\"score\"]\n",
    "        except WatsonApiException:\n",
    "            report = \"Error\"       \n",
    "    else:\n",
    "        report = NaN\n",
    "    return(report)\n",
    "\n",
    "def getNLU(x):\n",
    "    return (x if x % 10000 == 0 else x + 10000 - x % 10000)/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing to see if the `getSentiment` function worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = getSentiment(\"This camera worked qute well, I am really happy with its image quality and eas-of-use.\")\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"Electronics_meta.pickle\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the data to collect by subcategory and date. I.e. we want to see what are the reviews made for products in a particular subcategory on each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['sub_category_1','reviewTime','reviewText']]\n",
    "df = df.groupby(['reviewTime','sub_category_1']).agg(lambda x: \". \".join(x.tolist()))\n",
    "df['charlen']=list(map(lambda x:len(x),df.reviewText))\n",
    "df = df[df.charlen>20]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_cat = df\n",
    "senti_cat['NLU'] = list(map(lambda x:getNLU(x),senti_cat.charlen))\n",
    "senti_cat = senti_cat.sort_values(['NLU', 'charlen'], ascending=[1, 1])\n",
    "senti_cat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through our data, and store the results in a new column. Store the data in .pickle files for future reference. Here we decided to store many small pickles as we made Sentiment Analysis requests in smaller batches to ensure API credits were not wasted in case of server errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "for x in range(0,40000,1000):\n",
    "    batchN = senti_cat.iloc[x:x+1000, :]\n",
    "    t = datetime.datetime.now()\n",
    "    print (\"Start: \"+ t.strftime('%H:%M:%S'))\n",
    "    batchN['sentScore'] = list(map(lambda x:getSentiment(x),batchN.reviewText))\n",
    "    filename = \"catDate_sentScores_\"+str(int(x/1000))+\"k_\"+str(int((x+1000)/1000))+\"k.pickle\"\n",
    "    pickling_on = open(filename,\"wb\")\n",
    "    pickle.dump(batchN[['sentScore']], pickling_on)\n",
    "    pickling_on.close()\n",
    "    print(filename + \" complete\")\n",
    "\n",
    "batchN = senti_cat.iloc[40000:, :]\n",
    "t = datetime.datetime.now()\n",
    "print (\"Start: \"+ t.strftime('%H:%M:%S'))\n",
    "batchN['sentScore'] = list(map(lambda x:getSentiment(x),batchN.reviewText))\n",
    "filename = \"catDate_sentScores_\"+str(int(x/1000))+\"k_end.pickle\"\n",
    "pickling_on = open(filename,\"wb\")\n",
    "pickle.dump(batchN[['sentScore']], pickling_on)\n",
    "pickling_on.close()\n",
    "print(filename + \" complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Product Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
